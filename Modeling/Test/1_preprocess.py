# -*- coding: utf-8 -*-
"""1_preprocess.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1w1E0JjQCCf1-ESEci2KyBnqEj0YnLWxb
"""

#
import os
import json
import pandas as pd
from datetime import datetime

from collections import Counter

# Colab에서 khaiii 설치
!git clone https://github.com/kakao/khaiii.git
!pip install cmake
!mkdir build
!cd build && cmake /content/khaiii
!cd /content/build/ && make all
!cd /content/build/ && make resource
!cd /content/build && make install
!cd /content/build && make package_python
!pip install /content/build/package_python

def load_json(file_path: str):
    with open(file_path, 'r') as f:
        return json.load(f)

def most_popular(plylsts, key, n):
    counter = Counter()
    for plylst in plylsts:
        counter.update(plylst[key])
    counter_top_n= counter.most_common(n)
    song_tags, counts = zip(*counter_top_n)
    return counter_top_n, song_tags

# 0. (Gdrive) Datasets
from google.colab import drive


drive.mount('/content/drive')
train_json = load_json('/content/drive/MyDrive/[KDT]FinalProject/Modeling/Test/Datasets/test.json')
val_json = load_json('/content/drive/MyDrive/[KDT]FinalProject/Modeling/Test/Datasets/val.json')
test_json = load_json('/content/drive/MyDrive/[KDT]FinalProject/Modeling/Test/Datasets/test.json')
# train = train + val + test

train_pd = pd.DataFrame(train_json)
val_pd = pd.DataFrame(val_json)
test_json = pd.DataFrame(test_json)

train_pd.head()

# 0. (Local) Datasets
# if not (os.path.isdir('Datasets/')):
#     os.makedirs('Datasets/')

# train = load_json('Datasets/train.json')
# val = load_json('Datasets/val.json')
# test = load_json('Datasets/test.json')
# # train = train + val + test

# train_pd = pd.DataFrame(train_json)
# val_pd = pd.DataFrame(val_json)
# test_json = pd.DataFrame(test_json)

# train_pd.head()

# 1. Rank
data_by_year_month = train_pd.copy()
data_by_year_month['updt_year'] = data_by_year_month['updt_date'].map(lambda x: str(datetime.fromisoformat(x).year))
data_by_year_month['updt_month'] = data_by_year_month['updt_date'].map(lambda x: str(datetime.fromisoformat(x).month))

data_by_year_month.head()

# 2
def most_popular(plylsts, songs_or_tags, n):
    global most_popular_counter
    counter = Counter()
    for plylst in plylsts:
        counter.update(plylst)
    top_n = counter.most_common(n)
    most_popular_counter[songs_or_tags].update(counter)

    return [k for k, v in top_n]

most_n_songs = 100
most_n_tags = 50

_groupby = data_by_year_month.groupby(['updt_year', 'updt_month'])
most_popular_counter = {'songs': Counter(), 'tags': Counter()}
most_popular_songs = pd.DataFrame(_groupby.apply(lambda x: most_popular(x['songs'], 'songs', most_n_songs)))
most_popular_songs.columns = ['songs']
most_popular_tags = pd.DataFrame(_groupby.apply(lambda x: most_popular(x['tags'], 'tags', most_n_tags)))
most_popular_tags.columns = ['tags']

from khaiii import KhaiiiApi
from collections import defaultdict


def title_into_words(title):
    if title:
        khaiii_api = KhaiiiApi()
        analyzed = khaiii_api.analyze(test)
        morphs_list = []
        for word in analyzed:
            for morph in word.morphs:
                if POS[morph.tag]:
                    print(morph.lex, morph.tag)
                    morphs_list.append(morph.lex)
        return morphs_list
    else:
        return None


sentence = '잔잔한 피아노반주가 곁들여진 새벽감성음악'
POS = defaultdict(lambda: False)
_pos = ['XR', 'NNG']
for key in _pos:
    POS[key] = True

khaiii_api = KhaiiiApi()
analyzed = khaiii_api.analyze(test)
morphs_list = []
for word in analyzed:
    for morph in word.morphs:
        if POS[morph.tag]:
            print(morph.lex, morph.tag)
            morphs_list.append(morph.lex)
morphs_list

# 3.
print('split title into words...')


from khaiii import KhaiiiApi
from collections import defaultdict


def title_into_words(title):
    if title:
        khaiii_api = KhaiiiApi()
        analyzed = khaiii_api.analyze(title)
        morphs_list = []
        for word in analyzed:
            for morph in word.morphs:
                if POS[morph.tag]:
                    print(morph.lex, morph.tag)
                    morphs_list.append(morph.lex)
        return morphs_list
    else:
        return None


POS = defaultdict(lambda: True)
_pos = ['XR', 'NNG']
for key in _pos:
    POS[key] = True

data_by_year_month['title_words'] = data_by_year_month['plylst_title'].apply(title_into_words)

print('write train matrix...')
playlist_song_train_matrix = []
p_encode, s_encode, p_decode, s_decode = {}, {}, {}, {}
playlist_idx = 0
song_idx = 0
for q in train:
    if len(q['songs']) + len(q['tags']) + len(q['title_words']) >= 1:
        p_encode[q['id']] = playlist_idx
        for s in q['songs']:
            if s not in s_encode.keys():
                s_encode[s] = song_idx
                song_idx += 1
            playlist_song_train_matrix.append([playlist_idx, s_encode[s]])
        playlist_idx += 1
s_decode['@tag_start_idx'] = song_idx
for q in train:
    if len(q['songs']) + len(q['tags']) + len(q['title_words']) >= 1:
        for s in q['tags']:
            if s not in s_encode.keys():
                s_encode[s] = song_idx
                song_idx += 1
            playlist_song_train_matrix.append([p_encode[q['id']], s_encode[s]])
s_decode['@tag_title_start_idx'] = song_idx
for q in train:
    if len(q['songs']) + len(q['tags']) + len(q['title_words']) >= 1:
        for s in q['title_words']:
            if '!title_' + str(s) not in s_encode.keys():
                s_encode['!title_' + str(s)] = song_idx
                song_idx += 1
            playlist_song_train_matrix.append([p_encode[q['id']], s_encode['!title_' + str(s)]])
playlist_song_train_matrix = np.array(playlist_song_train_matrix)
playlist_song_train_matrix = coo_matrix((np.ones(playlist_song_train_matrix.shape[0]),(playlist_song_train_matrix[:,0], playlist_song_train_matrix[:,1])),shape=(playlist_idx,song_idx))
save_npz('data/playlist_song_train_matrix.npz', playlist_song_train_matrix)
for s in s_encode.keys():
    s_decode[s_encode[s]] = s
pickle_dump(s_decode, 'data/song_label_decoder.pickle')
pickle_dump(p_encode, 'data/playlist_label_encoder.pickle')

title_words_mp_counter, _ = most_popular(train, "title_words", 50)